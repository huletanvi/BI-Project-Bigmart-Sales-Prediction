# -*- coding: utf-8 -*-
"""BigMartSale.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1uUz7uh0gvDmTzOZrKTvTATBQnyampWgq
"""

from google.colab import drive
drive.mount('/content/drive/')

!pip install -U -q PyDrive
from pydrive.auth import GoogleAuth
from pydrive.drive import GoogleDrive
from google.colab import auth
from oauth2client.client import GoogleCredentials

auth.authenticate_user()
gauth = GoogleAuth()
gauth.credentials = GoogleCredentials.get_application_default()
drive = GoogleDrive(gauth)

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
get_ipython().magic('matplotlib inline')
import seaborn as sns
from sklearn.metrics import confusion_matrix
from sklearn.model_selection import train_test_split

downloaded = drive.CreateFile({'id':'1awH1DtKZXOxOtxaiP6ApBqr6KpkLHjCj'}) 
downloaded.GetContentFile('alg1.csv')

downloaded = drive.CreateFile({'id':'1vng5aSQNunDFvRPkMLD3ZblJxUukoKIk'}) 
downloaded.GetContentFile('alg2.csv')

downloaded = drive.CreateFile({'id':'1PioT7JpysR-eHiOkxOJH_DGWIdpdqv4K'}) 
downloaded.GetContentFile('alg3.csv')

downloaded = drive.CreateFile({'id':'16cKZSurNgvpoh48atUhvWbdZXOevu4X2'}) 
downloaded.GetContentFile('alg4.csv')

downloaded = drive.CreateFile({'id':'1cIBGtMSVy4jo5GmCQgmBGPEmiOUrXMcK'}) 
downloaded.GetContentFile('SampleSubmission.csv')

downloaded = drive.CreateFile({'id':'1ZuGqav6H3w2r43dZkii3VkFEVP991U_R'}) 
downloaded.GetContentFile('Test.csv')

downloaded = drive.CreateFile({'id':'1R-DMETmflEpONsjnLQ18dx1GRcfAO964'}) 
downloaded.GetContentFile('Train.csv')

train=pd.read_csv('Train.csv')

train.head()

train.shape

test=pd.read_csv('Test.csv')

test.head()

test.shape

train.isnull().sum()

test.isnull().sum()

train.describe()

#now plot correlation matrix
correl=train.corr()
ax=plt.subplots(figsize=(15,9))
sns.heatmap(correl,vmax=0.8,square=True)

train.head()

train.Item_Fat_Content.value_counts()  #need to optimize

train.Item_Type.value_counts()

train.Outlet_Identifier.value_counts()

train.Outlet_Size.value_counts()

train.Outlet_Location_Type.value_counts()

train.Outlet_Type.value_counts()

train.head()

train.isnull().sum()

train.Item_Weight.hist(bins=50)

train.Outlet_Size.hist(bins=50)

train.Outlet_Size.value_counts()

Item_Sales=train.Item_Outlet_Sales

data=train.append(test)

data.shape

data.isnull().sum()

data.isnull().sum()

correlation=data.corr()
sns.heatmap(correlation,vmax=.8,square=True)

data.apply(lambda x:len(x.unique()))

data.dtypes

data.dtypes.index

categorical_columns=[x for x in data.dtypes.index if data.dtypes[x]=='object']
categorical_columns

categorical_columns=[x for x in categorical_columns if x not in ['Item_Identifier','Outlet_Identifier']]
categorical_columns

#print frequencies of these categories
for col in categorical_columns:
    print('frequency of categories for variable')
    print(data[col].value_counts())

data.Item_Weight.fillna(data.Item_Weight.mean(),inplace=True)

#import mode function
from scipy.stats import mode

#determining the mode of each 
data.Outlet_Size=data.Outlet_Size.map({'Small':0,'Medium':1,'High':2})
outlet_size_mode = data.pivot_table(values='Outlet_Size', columns='Outlet_Type',aggfunc=(lambda x:mode(x).mode[0]) )
miss_bool = data['Outlet_Size'].isnull() 
data.loc[miss_bool,'Outlet_Size'] = data.loc[miss_bool,'Outlet_Type'].apply(lambda x: outlet_size_mode[x])

data.isnull().sum()

for i in data.dtypes.index:
    if len(data[i].value_counts())<30:
        print(i,"\n",data[i].value_counts())

data.pivot_table(index='Outlet_Type',values='Item_Outlet_Sales')

data.Item_Visibility.hist(bins=50)

data.Item_Visibility.mean()

data.loc[data['Item_Visibility']==0,'Item_Visibility']=data.Item_Visibility.mean()

data.Item_Type.value_counts()

data['Item_Type_Combined']=data.Item_Identifier.apply(lambda x:x[0:2])
data['Item_Type_Combined'].value_counts()

data['Item_Type_Combined']=data.Item_Type_Combined.map({'FD':'Food and Drinks','NC':'Non-Consumable','DR':'Drinks'})

data['Item_Type_Combined'].value_counts()

data['Outlet_Years']=2013-data['Outlet_Establishment_Year']
data['Outlet_Years'].describe()

data.Item_Fat_Content.value_counts()

data.Item_Fat_Content=data.Item_Fat_Content.replace({'LF':'Low Fat','reg':'Regular','low fat':'Low Fat'})
data.Item_Fat_Content.value_counts()

data.loc[data['Item_Type_Combined']=='Non-Consumable','Item_Fat_Content']='Non-Edible'

data.Item_Fat_Content.value_counts()

data.head()

#import library 
#now import labelEncoding
from sklearn.preprocessing import LabelEncoder
lb=LabelEncoder()
data['Outlet']=lb.fit_transform(data['Outlet_Identifier'])
var=['Item_Fat_Content','Outlet_Location_Type','Outlet_Type','Outlet_Size','Item_Type_Combined']
lb=LabelEncoder()
for item in var:
    data[item]=lb.fit_transform(data[item])

data.head()

data.drop(['Outlet_Establishment_Year','Item_Type'],inplace=True,axis=1)

data.head()

Item_Sales=data.Item_Outlet_Sales

train=data.iloc[:8523,:]
train.head()

test=data.iloc[8523:,:]

test.drop('Item_Outlet_Sales',inplace=True,axis=1)

test.head()



# A generalization function to prediction and file on sharing
target='Item_Outlet_Sales'
IDcol=['Item_Identifier','Outlet_Identifier']
from sklearn import model_selection ,metrics
def modelfit(alg,dtrain,dtest,predictor,target,IDcol,filename):
    alg.fit(dtrain[predictor],dtrain[target])
    prediction=alg.predict(dtrain[predictor])
    #now cross_validation
    cv_score=model_selection.cross_val_score(alg,dtrain[predictor],dtrain[target],cv=20,scoring='neg_mean_squared_error')
    cv_score=np.sqrt(np.abs(cv_score))
    print(np.sqrt(metrics.mean_squared_error(dtrain[target].values,prediction)))
    print("CV_SCORE : mean - %.4g | std - %.4g | max - %.4g | min - %.4g" % (np.mean(cv_score),np.std(cv_score),np.max(cv_score),np.min(cv_score)))
    dtest[target]=alg.predict(dtest[predictor])
    
    #now export on submission file 
    IDcol.append(target)
    submission=pd.DataFrame({x:dtest[x] for x in IDcol})
    submission.to_csv("C:\\Users\\naveen chauhan\\Desktop\\mldata\\mlp\\Big Mart Sale Prediction\\"+filename,index=False)

#Linear Regression on training set
from sklearn.linear_model import LinearRegression , Ridge,Lasso
predictor=[x for x in train.columns if x not in [target]+IDcol]
alg1=LinearRegression()
modelfit(alg1,train,test,predictor,target,IDcol,'alg1.csv')

predictors = [x for x in train.columns if x not in [target]+IDcol]
alg2 = Ridge(alpha=0.05,normalize=True)
modelfit(alg2, train, test, predictors, target, IDcol, 'alg2.csv')
coef2 = pd.Series(alg2.coef_, predictors).sort_values()
coef2.plot(kind='bar', title='Model Coefficients')

from sklearn.tree import DecisionTreeRegressor
predictors = [x for x in train.columns if x not in [target]+IDcol]
alg3 = DecisionTreeRegressor(max_depth=15, min_samples_leaf=100)
modelfit(alg3, train, test, predictors, target, IDcol, 'alg3.csv')
coef3 = pd.Series(alg3.feature_importances_, predictors).sort_values(ascending=False)
coef3.plot(kind='bar', title='Feature Importances')

predictors = ['Item_MRP','Outlet_Type','Outlet','Outlet_Years']
alg4 = DecisionTreeRegressor(max_depth=8, min_samples_leaf=150)
modelfit(alg4, train, test, predictors, target, IDcol, 'alg4.csv')
coef4 = pd.Series(alg4.feature_importances_, predictors).sort_values(ascending=False)
coef4.plot(kind='bar', title='Feature Importances')

y=train.Item_Visibility
train = train.drop(labels=['Item_Visibility'],axis=1)
train.head()

from sklearn.model_selection import train_test_split
x_train,x_val,y_train,y_val = train_test_split(train,y,test_size=0.3)
print(x_train.shape)
print(y_train.shape)
print(x_val.shape)
print(y_val.shape)